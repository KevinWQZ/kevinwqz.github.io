<!DOCTYPE html>
<html>

  
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="baidu-site-verification" content="vuqzu4486i" />
  <meta name="baidu-site-verification" content="yR54QJzaFG" />
  <title>Python 萌新 - 花10分钟学爬虫</title>
  <meta name="description" content="">
  <meta name="author" content="xie tao">

<!--   <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python 萌新 - 花10分钟学爬虫">
  <meta name="twitter:description" content=""> -->
  
<!--   <meta property="og:type" content="article">
  <meta property="og:title" content="Python 萌新 - 花10分钟学爬虫">
  <meta property="og:description" content=""> -->
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.ico" />
  <link href="/assets/images/favicon.ico" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2018/01/ScrapySpider/">
  <link rel="alternate" type="application/rss+xml" title="wangquanzhen" href="http://localhost:4000/feed.xml">
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?df0994aacf7a4c9779a5632908de3196";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 wangquanzhen 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="wangquanzhen logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="前往 wangquanzhen 的主页" class="blog-button">wangquanzhen</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">探索之旅，学习之路</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">你好，我是王权振！欢迎来到我的个人主页.</br>90后代码搬运工，</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="进入博客" class="blog-button">博客</a></li>
                
                  <li class="navigation__item"><a href="/about/index.html" target="_blank" title="了解更多关于我">关于</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
  
    <a href="http://weibo.com/u/2803559240" title="xietao3 的微博" target="_blank">
    <!-- <a href="http://weibo.com/xietao3" title="xietao3 的微博" target="_blank"> -->
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/xietao3" title="xietao3 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  
   <!-- CSDN -->
  <li class="navigation__item">
    <a href="http://blog.csdn.net/xietao3" title="xietao3 的 CSDN" target="_blank">
      <img src="/assets/images/csdn.png" width="20" height="20" />
      <!-- <i class='social fa fa-edit'></i> -->
      <span class="label">csdn</span>
    </a>
  </li>
  

  <!-- RSS -->
<!--   <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li> -->

  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-orange"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2018-01-26 23:40:11 +0800" itemprop="datePublished" class="post-meta__date date">2018-01-26</time> &#8226; <span class="post-meta__tags tags"></span>
    </div>
    <h1 class="post-title">Python 萌新 - 花10分钟学爬虫</h1>
  </header>

  <section class="post">
    <p><img src="https://user-gold-cdn.xitu.io/2018/1/26/1613309f74eee5df?imageView2/1/w/1304/h/734/q/85/interlace/1" alt="图片来自网络" /></p>

<blockquote>
  <p>Python 新手入门很多时候都会写个爬虫练手，本教程使用 Scrapy 框架，帮你简单快速实现爬虫，并将数据保存至数据库。在机器学习中数据挖掘也是十分重要的，我的数据科学老师曾经说过，好算法不如好数据。</p>
</blockquote>

<h1 id="介绍">介绍</h1>

<p><a href="https://scrapy.org/">Scrapy</a> ，Python 开发的一个快速、高层次的屏幕抓取和 web 抓取框架，用于抓取 web 站点并从页面中提取结构化的数据。文件结构清晰，即使是小白也能够快速上手，总之非常好用😂。</p>

<p><a href="https://baike.baidu.com/item/XPath/5574064?fr=aladdin">XPath</a> ,它是一种用来查找 XML 文档中节点位置的语言。 XPath 基于 XML 的树状结构，有不同类型的节点，包括元素节点，属性节点和文本节点，提供在数据结构树中找寻节点的能力。</p>

<p><a href="https://www.mysql.com/">MySQL</a> 是一种关系数据库管理系统，它的优势：它是免费的。作者是下载了 <a href="https://www.mamp.info/en/">MAMP for Mac</a> ，内嵌<code class="highlighter-rouge">MySQL</code>和<code class="highlighter-rouge">Apache</code>。</p>

<p>首先通过 Scrapy 爬取到网页后， 通过 XPath 来定位指定元素，获取到你想要的数据,得到数据之后可以将数据存入数据库( MySQL )。简单了解之后就可以开始编写你的爬虫。</p>

<p><strong>*重要</strong>：下载并查看 <a href="https://github.com/xietao3/ScrapySample">Demo</a> ，结合本文可以快速实现一个基本爬虫✌️。</p>

<h1 id="准备工作">准备工作</h1>

<p>安装 Scrapy <code class="highlighter-rouge">(系统默认安装了 Python)</code>:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ pip install Scrapy
</code></pre></div></div>
<p>在当前目录新建工程</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ scrapy startproject yourproject
</code></pre></div></div>
<p>新建工程文件结构如下：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yourproject/
----|scrapy.cfg             # 部署配置文件
    |yourproject/           # 工程目录
    |----__init__.py
    |----items.py           # 项目数据文件
    |----pipelines.py       # 项目管道文件
    |----settings.py        # 项目设置文件
    |----spiders/           # 我们的爬虫 目录
        |----__init__.py    # 爬虫主要代码在这里    
</code></pre></div></div>

<p>简单的爬虫主要使用了<code class="highlighter-rouge">spiders</code>、<code class="highlighter-rouge">items</code>、<code class="highlighter-rouge">pipelines</code>这三个文件：</p>

<ul>
  <li>spider ：爬虫的主要逻辑。</li>
  <li>items ：爬虫的数据模型。</li>
  <li>pipelines ： 爬虫获取到的数据的加工工厂，可以进行数据筛选或保存。</li>
</ul>

<h1 id="数据模型items">数据模型：items</h1>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/26/16132e37082a8831?w=300&amp;h=300&amp;f=jpeg&amp;s=12764" alt="items" /></p>

<p>先看看我们要爬取的<a href="http://quotes.toscrape.com">网站</a>，这个是 Scrapy 官方 Demo 爬取数据用的网站，我们先用这个来练手。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/24/16128781a79a30f4?w=1774&amp;h=1134&amp;f=jpeg&amp;s=250425" alt="quotes" /></p>

<p>分析网页的信息我们可以看到网页主体是一个列表，列表每一行都包含可一句引用、作者名字、标签等信息。作者名右边点击（about）可以看到作者的详细信息，包括介绍、出生年月日、地点等等。根据上面的数据，我们可以先创建如下数据模型：</p>

<p><strong>items.py</strong></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import scrapy

# quote 我们要爬取的主体
class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    tags = scrapy.Field()
    author = scrapy.Field()

    next_page = scrapy.Field()
    pass
    
# quote 的作者信息 对应 QuoteItem.author
class AuthorItem(scrapy.Item):
    name = scrapy.Field()
    birthday = scrapy.Field()
    address = scrapy.Field()
    description = scrapy.Field()
    pass
</code></pre></div></div>

<p>所有的模型必须继承<code class="highlighter-rouge">scrapy.Item</code>，完成这一步我们就可以开始写爬虫的逻辑了。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 完整的 QuoteItem 数据结构示例
{
    text,
    tags,
    author:{
        name,
        birthday,
        address,
        description
    }
}
</code></pre></div></div>

<h1 id="爬虫spider">爬虫：spider</h1>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/26/16132e022f073f10?w=200&amp;h=200&amp;f=jpeg&amp;s=17148" alt="Spider" /></p>

<p>既然是爬虫，自然需要去爬取网页，爬虫部分的几个要点：</p>

<ol>
  <li>引入你创建的数据模型</li>
  <li>首先爬虫类要继承<code class="highlighter-rouge">scrapy.Spider</code>。</li>
  <li>设置爬虫的名字<code class="highlighter-rouge">name</code>，启动爬虫时要用。</li>
  <li>将你要爬取的网址放入<code class="highlighter-rouge">start_requests()</code>，作为爬虫的起点。</li>
  <li>爬取的网页信息成功后，在的请求响应<code class="highlighter-rouge">parse()</code>中解析。</li>
</ol>

<p><strong>spiders/__init__.py</strong></p>

<ul>
  <li><strong>在顶部引入创建的数据模型。</strong></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import scrapy
from ScrapySample.items import QuoteItem
from ScrapySample.items import AuthorItem
</code></pre></div></div>

<ul>
  <li><strong>爬虫类，<code class="highlighter-rouge">name</code>-&gt;爬虫名字，<code class="highlighter-rouge">allowed_domains</code>-&gt;爬取网页的白名单。</strong></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class QuotesSpider(scrapy.Spider):
    name = "quotes"
    allowed_domains = ["quotes.toscrape.com"]
</code></pre></div></div>

<ul>
  <li>
    <p><strong>在<code class="highlighter-rouge">start_requests()</code>中记录你要爬取的网址。</strong></p>

    <p>可以只放入一个网址，然后让爬虫自己爬取起始网址中下一页的链接。也可以在这里把所有需要爬的网址直接放入，比如说<code class="highlighter-rouge">page</code>一般是从1开始，并且有序的，写一个<code class="highlighter-rouge">for</code>循环可以直接输入所有页面的网址。</p>

    <p>本文使用的是让爬虫自己去爬取下一页网址的方式，所以只写入了一个起始网址。</p>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
</code></pre></div></div>

<ul>
  <li>
    <p><strong>如下代码，爬取网页成功之后，我们要分析网页结构，找到我们需要的数据。</strong></p>

    <p>我们先来看XPath语法，<code class="highlighter-rouge">//div[@class="col-md-8"]/div[@class="quote"</code>：这是表示查找 class 为<code class="highlighter-rouge">"col-md-8"</code>的 div 节点下的一个子节点，并且子节点是一个 class 为<code class="highlighter-rouge">"quote"</code> div 节点。如果在当前页面找到了这样一个节点，则返回节点信息，如果没有找到则返回<code class="highlighter-rouge">None</code>。</p>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def parse(self, response):
        # 通过查看器，找到列表所在的节点
        courses = response.xpath('//div[@class="col-md-8"]/div[@class="quote"]')

        for course in courses:
            # 将数据模型实例化 并从节点中找到数据填入我们的数据模型
            item = QuoteItem()
            # 轮询 course 节点下所有 class 为 "text" 的 span 节点,获取所有匹配到的节点的 text() ，由于获取到的是列表，我们默认取第一个。
            item['text'] = course.xpath('.//span[@class="text"]/text()').extract_first()
            item['author'] = course.xpath('.//small[@class="author"]/text()').extract_first()
            item['tags'] = course.xpath('.//div[@class="tags"]/a/text()').extract()

            # 请求作者详细信息
            author_url = course.xpath('.//a/@href').extract_first()
            # 如果作者介绍的链接不为空 则去请求作者的详细信息
            if author_url != '':
                request = scrapy.Request(url='http://quotes.toscrape.com'+author_url, dont_filter=True, callback=self.authorParse)
                # 将我们已经获取到的 QuoteItem 传入该请求的回调函数 authorParse()，在该函数内继续处理作者相关数据。
                request.meta['item'] = item
                yield request
        
        # 继续爬向下一页 该函数具体实现下面会分析
        next_page_request = self.requestNextPage(response)
        yield next_page_request
</code></pre></div></div>

<p><em>这段注释不是很详细，如果看不懂可能需要补一下相关知识。</em></p>

<ul>
  <li>
    <p><strong>爬取作者详细信息</strong></p>

    <p>成功获取作者详细信息 AuthorItem 后并且赋值给 QuoteItem 的属性 <code class="highlighter-rouge">author</code> ，这样一个完整的引述信息 QuoteItem 就组装完成了。</p>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def authorParse(self, response):
        # 先获取从 parse() 传递过来的 QuoteItem
        item = response.meta['item']
        # 通过查看器，找到作者详细信息所在节点
        sources = response.xpath('//div[@class="author-details"]')
        
        # 实例化一个作者信息的数据模型
        author_item = AuthorItem()
        # 往作者信息模型填入数据
        for source in sources:
            author_item['name'] = source.xpath('.//h3[@class="author-title"]/text()').extract_first()
            author_item['birthday'] = source.xpath('.//span[@class="author-born-date"]/text()').extract_first()
            author_item['address'] = source.xpath('.//span[@class="author-born-location"]/text()').extract_first()
            author_item['description'] = source.xpath('.//div[@class="author-description"]/text()').extract_first()
    
        # 最后将作者信息 author_item 填入 QuoteItem 
        item['author'] = author_item
        # 保存组装好的完整数据模型
        yield item
</code></pre></div></div>

<ul>
  <li>
    <p><strong>爬虫自己找到出路（下一页网页链接）</strong></p>

    <p>通过查看器我们可以找到<code class="highlighter-rouge">下一页</code>按钮元素，找到该节点并提取链接，爬虫即奔向下一个菜园。</p>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def requestNextPage(self, response):
        next_page = response.xpath('.//li[@class="next"]/a/@href').extract_first()
        # 判断下一个是按钮元素的链接是否存在
        if next_page is not None:
            if next_page != '':
                return scrapy.Request(url='http://quotes.toscrape.com/'+next_page, callback=self.parse)
        return None
</code></pre></div></div>

<p>爬虫的主要逻辑到这里就结束了，我们可以看到，一小段代码就可以实现一个简单的爬虫。一般主流网页都针对防爬虫做了一些处理，实操过程中也许并不会这么顺利，我们可能需要模仿浏览器的User-Agent，或者做访问延时防止请求过于频繁等等处理。</p>

<h1 id="数据处理pipelines">数据处理：pipelines</h1>

<p>pipelines是 Scrapy 用来后续处理的管道，可以同时存在多个，并且可以自定义顺序执行，通常用来做数据处理和数据保存。我们需要在<code class="highlighter-rouge">settings.py</code>文件中设置需要需要执行的管道和执行顺序。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 在 settings.py 加入下面的代码
ITEM_PIPELINES = {
   'ScrapySample.pipelines.ScrapySamplePipeline': 300,
}
</code></pre></div></div>
<p>在这里我只使用了一个管道<code class="highlighter-rouge">ScrapySamplePipeline</code>，用来将数据保存到数据库当中，后面的数字<code class="highlighter-rouge">300</code>是表示该管道的优先级，数字越小优先级越高。</p>

<p>由于我们要保存数据到数据库，所以我们需要先在本地搭建起数据库服务，我这里用的是<code class="highlighter-rouge">MySQL</code>，如果没有搭建的小伙伴可以下个 <a href="https://www.mamp.info/en/">MAMP</a> 免费版本，安装好傻瓜式操作一键启动<code class="highlighter-rouge">Apache</code>、<code class="highlighter-rouge">MySQL</code>服务。当然，数据库和表还是要自己建的。</p>

<p><img src="https://user-gold-cdn.xitu.io/2018/1/26/16132d56bcd4d82e?w=300&amp;h=207&amp;f=png&amp;s=16004" alt="MAMP" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 在 pipelines.py 中加入数据库配置信息
config = {
    'host': '127.0.0.1',
    'port': 8081,
    'user': 'root',
    'password': 'root',
    'db': 'xietao',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor,
}
</code></pre></div></div>

<p>我们可以在<code class="highlighter-rouge">__init__()</code>函数里做一些初始化工作，比如说连接数据库。</p>

<p>然后<code class="highlighter-rouge">process_item()</code>函数是管道处理事件的函数，我们要在这里将数据保存入数据库，我在这个函数里写了一些插入数据库操作。</p>

<p><code class="highlighter-rouge">close_spider()</code>函数是爬虫结束工作时候调用，我们可以在这里关闭数据库。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class ScrapySamplePipeline(object):

    def __init__(self):
        # 连接数据库
        self.db = sql.connect(**config)
        self.cursor = self.db.cursor()

    def process_item(self, item, spider):
        # 先保存作者信息
        sql = 'INSERT INTO author (name, birthday, address, detail) VALUES (%s, %s, %s, %s)'
        self.cursor.execute(sql, (item['author']['name'], item['author']['birthday'], item['author']['address'], item['author']['description']))
        # 获取作者id
        author_id = self.cursor.lastrowid

        # 保存引述信息
        sql = 'INSERT INTO spider (text, tags, author) VALUES (%s, %s, %s)'
        self.cursor.execute(sql, (item['text'], ','.join(item['tags']), author_id))
        self.db.commit()

    # 即将结束爬虫
    def close_spider(self, spider):
        self.db.close()
        self.cursor.close()
        print('close db')
</code></pre></div></div>

<p>如果不需要保存数据库或者对数据处理的话，<code class="highlighter-rouge">pipelines</code>这部分是可以忽略的。这个时候在命令行切换到工程目录下，输入开始执行爬虫命令：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ scrapy crawl quotes
</code></pre></div></div>

<p>部分不保存到数据库的小伙伴可以使用下方命令，将爬取的数据以 Json 格式导出到该工程目录下。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ scrapy crawl quotes -o quotes.json
</code></pre></div></div>

<p>最后贴上数据库数据成功录入的截图。
<img src="https://user-gold-cdn.xitu.io/2018/1/26/161330320877c008?w=2014&amp;h=1034&amp;f=png&amp;s=555547" alt="Data" /></p>

<h1 id="总结">总结</h1>

<p>这是作者最开始学习 Python 的时候写的，有一些不尽人意的地方后面会再调整，写下本文用意是巩固知识或是用于以后回顾，同时希望对同样刚开始学习 Python 的读者有所帮助。</p>

<p><strong>最后再次贴上<a href="https://github.com/xietao3/ScrapySample">Demo</a> ️。</strong></p>


  </section>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2018/05/ReactNativeLearningNotes/" title="link to 一份传男也传女的 React Native 学习笔记">一份传男也传女的 React Native 学习笔记</a></h2>
       <p class="excerpt">  学习就如同长跑，要想跑的远，就不能跑得太快。背景介绍这段时间了解了一些前端方面的知识，并且用 React Native 写了一个简易新闻客户端 Demo。React Native 和原生开发各有所长，具体就不细说。混合使用能充分发挥各自长处，唯一的缺憾就是 React Native 和原生通信过程相对不那么友好。在这里分享一下学习过程中个人认为比较重要的知识点和学习资料，本文尽量写得轻一些，希望对读者能够有所帮助。预备知识有些前端经验的小伙伴学起 React Native 就像老马识途...&hellip;</p>
       <div class="post-list__meta"><time datetime="2018-05-12 21:40:11 +0800" class="post-list__meta--date date">2018-05-12</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2018/05/ReactNativeLearningNotes/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2018/01/XTImageDownloader/" title="link to Python 萌新 - 实现 Markdown 图片下载器">Python 萌新 - 实现 Markdown 图片下载器</a></h2>
       <p class="excerpt">  简书支持打包下载所有文章功能，可以方便作者转移或保存。但是图片不支持自动下载，最近在学Python，便写了一个md图片下载器。目标本人 Python 新手，欢迎大佬指点。本文主要是对源码进行解读，期望实现以下目标：  一键下载所有Markdown文件中的图片，并保存到本地。  图片根据文章分类  简单易用。先上最终效果:实现步骤  搜索指定文件夹，找出文件夹及子文件包含的md文件。  匹配出md文件中所有的图片。  所有图片异步下载。  下载报告与GUI。  Python 打包工具。1...&hellip;</p>
       <div class="post-list__meta"><time datetime="2018-01-19 23:40:11 +0800" class="post-list__meta--date date">2018-01-19</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2018/01/XTImageDownloader/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
   
    <div id="gitalk-container"></div>
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

    <script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'a7e3bcc8c313f6fcf2cf',
        clientSecret: '1d7f990ff0250384dea6b037ce7d6e455d84d472',
        repo: 'xietao3.github.io',
        owner: 'xietao3',
        admin: ['xietao3'],
        id: 'BlogComment',
        distractionFreeMode: true
      });
      gitalk.render('gitalk-container');

    </script>

  



  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">本站由 <a href="http://xietao3.com/about">xietao3</a> 
        创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2016</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
